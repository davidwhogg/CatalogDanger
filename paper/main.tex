\documentclass[10pt]{article}
\usepackage[letterpaper]{geometry}
\usepackage{xcolor}
\usepackage{amsmath}

% Margins and typesetting
\setlength{\textwidth}{5.00in}
\setlength{\oddsidemargin}{3.25in}
\addtolength{\oddsidemargin}{-0.5\textwidth}
\addtolength{\topmargin}{-0.55in}
\addtolength{\textheight}{2.00in}
\linespread{1.08}
\pagestyle{myheadings}
\markboth{}{\color{gray}\sffamily Hogg / What is a catalog?}
\sloppy\sloppypar\raggedbottom\frenchspacing

% Math
\newcommand{\given}{\,|\,}
\newcommand{\dd}{\mathrm{d}}

\title{\textbf{What is a catalog?}}
\author{David W. Hogg}
\date{Draft in progress as of \today}

\begin{document}

\maketitle\thispagestyle{empty}

\begin{abstract}\noindent
    Observational astronomy projects often produce catalogs---of stars, galaxies, quasars, planet hosts, and so on---for use in other projects.
    How can we use a catalog responsibly to understand the population of objects under study?
    The answer to this turns out to depend on how the catalog was made.
    In particular, if the catalog entries were obtained by operations on a set of (nearly) independent or separable likelihood functions, the catalog can be used much more straightforwardly than if the catalog entries were obtained by a supervised method or operations on a posterior pdf or a method that involves implicitly or explicitly population-level properties.
    This is true no matter whether the subsequent analyses of the catalog are Bayesian or frequentist.
    Importantly, at the present day, many important catalogs are being made from the outputs of MCMC runs or supervised, discriminative machine-learning methods (classifications or regressions).
    These catalogs are very hard or even impossible to use for population studies.
    I demonstrate these points mathematically, and also with toy examples from cosmology, stars, and exoplanets.
    I recommend that catalogs be designed and made with the feasibility of particular end-user investigations as explicit requirements.
\end{abstract}

\section{Introduction}

Consider the following (true) story:
Scientific Team~A has a set of $10^5$ well-measured stellar metallicities from a spectroscopic survey, such as \textsl{APOGEE} \cite{apogee}.
Team A wants a much bigger catalog, so they obtain $>10^7$ low-resolution (XP) spectra of stars from the ESA \textsl{Gaia} Mission \cite{gaiadr3}.
They use the \textsl{APOGEE--Gaia} overlap as a training set to train a regression; this regression delivers a function $f(\cdot)$ that takes as input a \textsl{Gaia} XP spectrum and outputs a stellar metallicity [Mg/H].
It works well and delivers accurate predictions (on, say, held-out data from the training set).
Scientific Team~B wants to look at the metallicities of stars as a function of orbit and orbital phase in the Milky Way disk.
This is good: Team~A has a customer.
Team~A delivers to Team~B a huge catalog of stellar metallicities.

Team~B uses this catalog to make plots of the metallicities of stars as a function of position and velocity in the Milky Way disk.
They find that stars in different Galactic positions have different metallicities.
For example, they find that the metallicity is a strong function of latitude and also a strong function of vertical action.
Unfortunately, they also find that the metallicity is a strong function of latitude \emph{at fixed vertical action}.
That's impossible: Stars don't stay at one latitude over time; their latitudes change as they orbit.
Their vertical actions are (close to) invariant with time.
Metallicity ought to depend on action at fixed latitude, but not on latitude at fixed action.

So what went wrong?
What went wrong is that the catalog made by Team~A was made by a regression, in which the imputed labels for the non-training-set stars are set by a machine-learning method that (effectively) puts similar labels onto stars with similar spectra.
That sounds like a good idea, until the problem is looked at from a perspective of \emph{causal inference}.
If, in the training set, there are stellar properties that are covariant with metallicity, or confounders with metallicity, then the regression can easily ``learn'' the metallicities by learning from the covariant property.
Higher metallicity stars tend to be at lower latitudes in the training set, so they tend to be more dust reddened, so a reddened spectrum tends to be of a higher metallicity star (in the training set).
The regression (correctly) learns this (true) fact and uses it.
But then the catalog is not safe for studies of stellar abundances as a function of stellar orbit.

Generalizing a bit further, what went wrong?
The outputs of discriminative regressions and classifications (like the regression by Team~A) are effectively \emph{posterior estimates} of labels, with the population training set serving as an effective, empirical prior pdf.
The regression doesn't know what features in the spectrum are salient to metallicity; it only knows that stars of similar spectra should be given similar labels.
Thus the regression can (and should) use non-metallicity information to infer metallicity labels, provided that the correlations are reliable in the population that was observed for the training and validation sets.
As we will discuss below, Team B needs individual-star likelihood estimates that are independent of population-level correlations, because they want to measure properties of the population.
They can't use posterior estimates based on training-set population priors to do their science.
Even if the training set was \emph{exactly} the same as the full final catalog in all properties, the measurements would not safe to use even for individual objects, because the metallicity estimates are being made with reddening information.

Note that these problems with the Team-A catalog are not appearing because the regression by Team~A was done badly.
Nor is it that Team~A had incorrect inputs.
Even if everything Team~A did was done perfectly, and passed all tests with held-out data, Team B will still find the catalog (na\"ively) unusable for their scientific goals.
The catalog does everything Team~A expected it to do.
But real data sets have covariances among properties such that no na\"ive regression performed by Team~A will deliver a catalog that is safe for use by Team~B.

In what follows we will elucidate how catalogs are used in astrophysics to make statements about populations of objects, and thereby develop requirements on catalogs to make these uses safe.
It will turn out that in order for catalogs to be useful for population studies, it is a requirement that the user be given---or be able to obtain---information that permits construction of likelihood functions or estimators that do not contain population-level prior information.
This is not the only way to do population-level science; we also discuss alternatives to catalogs that might be more general.

\section{Catalog and inference generalities}

Imagine that some data-taking project has obtained some kind of valuable data object $y_n$ for every source $n$ in a set of $N$ sources.
This data object $y_n$ might be a spectrum, or some photometry, or a light curve.
For now we will assume that all of these data objects are the same size or shape, but nothing we do going forward will depend strongly on this assumption.
If you want a concrete picture, imagine that each source $n$ is a star, and the corresponding data object $y_n$ is a 10,000-pixel spectrum of that star.

Now imagine that each star $n$ has important, scientifically valuable latent properties $\alpha_n$, which are not directly observable, but which can be inferred from the data object $y_n$.
For concreteness, $\alpha_n$ might be the age, mass, and composition (element abundances)\footnote{There is an old---and substantially true---classical conjecture that all of the properties of a star can be predicted from its age, mass, and composition.} of the star $n$.

In many situations, you don't have (or don't want) access to the data objects $y_n$.
Instead you just get (or want) estimates $\hat{\alpha}_n$ of the latent parameters.
For example, you might get an estimate of the age, mass, and composition for each star $n$.
That might be called \emph{a catalog} of stellar parameters.
In the best-case scenario, this catalog also includes an uncertainty variance estimate $\hat{\Sigma}_n$ for each source, which is maybe an uncertainty variance tensor corresponding to the variance of the estimator that produced the estimate $\hat{\alpha}_n$.
Sometimes catalogs include much more sophisticated outputs, like samplings or posterior probability distributions; we will return to these possibilities below.

What are these catalog entries $\hat{\alpha}_n$, and how can we use them?
One (nearly ideal, frequentist) possibility is that the estimates $\hat{\alpha}_n$ and the uncertainties $\hat{\Sigma}_n$ give some approximate representation of an individual-object \emph{likelihood function} for object $n$.
The individual-object likelihood function is $p(y_n\given\alpha)$, which is the probability density for the data given the latent parameters, evaluated at the empirically observed data.
This function can be constructed when we have a full \emph{forward model} or physics-driven \emph{generative model} of the data $y_n$, including the noise.
That is, maybe each estimate $\hat{\alpha}_n$ and uncertainty variance $\hat{\Sigma}_n$ are given by
\begin{align}
    \hat{\alpha}_n & \leftarrow \arg\max_{\alpha} p(y_n\given\alpha) \\
    \hat{\Sigma}_n & \leftarrow \left[-\frac{\dd^2}{\dd\alpha^2}\ln p(y_n\given\alpha)\right]_{\hat{\alpha}_n}^{-1} ~.
\end{align}
These are reasonable choices, when the likelihood function is tractable.
In the not-uncommon case that the likelihood (seen as a function of the latents $\alpha$) is approximately proportional to a Gaussian function, these two parameters set the location and width (volume) of that Gaussian function.
More generally, we will think of $\hat{\alpha}_n$ as some kind of estimator made by (approximately) optimizing a likelihood function, or some approximation thereto.

In the Bayesian case, these catalog entries might be related to an individual-object \emph{posterior pdf} instead.
The individual-object posterior pdf is $p(\alpha\given y_n)$, which is the probability density for the latent $\alpha$ given the data.
This function can be constructed when we have the likelihood function plus an individual-object prior pdf $p(\alpha\given\theta^\ast)$;
\begin{align}
    p(\alpha\given y_n) &= \frac{1}{Z_n}\,p(y_n\given\alpha)\,p(\alpha\given\theta^\ast) ~,
\end{align}
where $Z_n$ is a normalization constant to make the units or normalization integrals work out,
and $\theta^\ast$ is a set of parameters or decisions or hyper-parameters describing the properties of the prior pdf.
In this case, each catalog entry $\tilde{\alpha}_n$ might be a mean-of-posterior estimate, and the uncertainty variance estimate $\tilde{\Sigma}_n$ might be a variance-of-posterior estimate:
\begin{align}
    \tilde{\alpha}_n & \leftarrow \int \alpha\,p(\alpha\given y_n)\,\dd\alpha \\
    \tilde{\Sigma}_n & \leftarrow \int [\alpha - \tilde{\alpha}_n]^2\,p(\alpha\given y_n)\,\dd\alpha ~.
\end{align}
In the not-uncommon case that the posterior pdf is approximately Gaussian, these two parameters set the location and width (volume) of that Gaussian.
More generally, we will think of the Bayesian estimator $\tilde{\alpha}_n$ as some kind of estimator made by taking a moment or optimum of a posterior pdf, or some approximation thereto.

In both cases---frequentist and Bayesian---the catalog contains, for each star $n$, an estimate of the latents and an uncertainty variance thereon.
In the frequentist case, these are descriptions or indicators of the likelihood function.
In the Bayesian case, these are descriptions or indicators of the posterior pdf.

Sometimes there are nuisance parameters.
For example.... HOGG CALIBRATION.
Marginalized likelihoods.
Profile likelihoods.

What if I get a sampling or a pdf rather than a value for the latents?

What if I only care about the age? What do I do?

Now what happens if they make the catalog using a discriminative ML regression method?

Now what happens if they make the catalog using a generative ML model?

Note that we are assuming everywhere that everyone has done everything right.
We are not criticizing bad methods of making catalogs.
We are exposing the point that even good catalogs can make for bad population inferences.

\section{Population inferences}

Simplest possible inference: Take the mean age of stars in some part of phase space.

\section{Examples}

\subsection{Why does \textsl{Gaia} have negative parallaxes?}

\subsection{Lutz--Kelker corrections}

\subsection{Sodium abundances in \textsl{APOGEE}}

\subsection{What else?}

\section{Objections and exceptions}

\subsection{But what if I am using flat priors?}

\subsection{What if test is statistically identical to train?}

\subsection{What if my prior is only over nuisance parameters?}

\subsection{What else?}

\section{Recommendations and discussion}

\paragraph{Acknowledgements}
It is a pleasure to thank
  Neige Frankel (CITA)
for valuable discussions and input.

\bibliographystyle{plain}
\raggedright
\bibliography{dipole}

\end{document}
